---
title: "Weight Lifting Exercise Prediction"
author: "Kirby Zhang"
date: "Monday, October 20, 2014"
output: html_document
---
#Abstract
[Weight lifting data](http://groupware.les.inf.puc-rio.br/har) collected by Ugulino, Velloso, and Fuks is used to form a prediction model. The model classifies one of five barbell lifting methods based on accelerometer and gyro data recorded on the user. Using manual feature selection and cross validation within and between different modeling methods, a high degree of prediction accuracy is achieved.


#Exploratory Analysis
We will go over the steps which were taken to preprocess the dataset and analyze its features (the data was obtained from [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)).

##Tidy Data
```{r,results='hide',echo=TRUE,cache=FALSE,warning=FALSE,message=FALSE}
library(knitr)
library(gbm)
library(caret)
library(lubridate)
library(plyr)
library(dplyr)
library(doParallel)

opts_chunk$set(warning=FALSE,message=FALSE)
```

```{r,cache=TRUE}

setClass("quoted.numeric")
setAs("character", "quoted.numeric", function(from) as.numeric(gsub('"','',from)))
df <- read.csv("pml-training.csv",colClasses=c(
  "character",
  "factor",
  rep("character", 4),
  "integer",
  rep("quoted.numeric", 159-8 + 1),
  "factor"
))
```
The dataset consisted of `r dim(df)[1]` observations with one outcome variable and 160 features. We created a "quoted.numeric" class to help us strip away quotation marks surrounding numeric values. A summary of the dataframe showed that columns 2 and 160 are appropriate as factor variables:
```{r,cache=TRUE}
summary(df[,c(2,160)])
```
##Feature Selection
Next we took a closer look at the 160 features. We start by looking at the "testing" dataset we received. We reasoned that features that are uninteresting in this small 20-element dataset cannot be useful predictor variables. 
```{r,cache=TRUE}
df_submit <- read.csv("pml-testing.csv")
dmy_hm(df$cvtd_timestamp) -> df$time
dmy_hm(df_submit$cvtd_timestamp) -> df_submit$time
select(df_submit, X, user_name, time, new_window, num_window) %>% arrange(user_name) 
```

We were able to eliminate `new_window` (no variation) as well as the variable `X` (a simple row count variable).

###Timestamp and Window Data
Next we are interested in looking at the timestamp fields as well as the `num_window` variable.
```{r,fig.height=3}
qplot(df$time, df$num_window, color=df$classe, 
      main="time appears to be an experiment artifact")
qplot(df$num_window, fill=df$classe, main="num_window appears distributed across classes")
qplot(df$num_window, fill=df$user_name, main="num_window appears to be an artifact\n associated with individual users")
qplot(num_window, classe, color=user_name, data=df, main="num_window appears related to the combination\nof user_name and activity classe")
```

Our first find was that the timestamp fields correspond to when the researchers conducted the weight lifting experiments. They should not be used in prediction. On the other hand, we found the `num_window` feature to be interesting. Even though the variable seems to be a data collection artifact and not likely to be a result of user activity, in the last plot above its value seems to be related to the classe on a per-user_name basis. We will therefore explore models both with and without this feature. 

###Predictor Variables
Next, we looked at the remaining columns corresponding acceleration and gyro data taken at different body positions. Without more domain specific knowledge (in the weigh lifting exercise domain), we chose to focus on eliminating inconsistent or low quality data fields. We found them by isolating the large number fields which contained NA's in the testing dataset. 

We isolated columns which contained purely NA in the testing set.
```{r}
bad_cols <- which(apply(is.na(df_submit), 2, sum)/nrow(df_submit) == 1)
```

We found `r length(bad_cols)` columns we identified as unusable due to high sparsity of data (100% in the testing data set), and we confirmed their unusability by finding that all of these same columns have greater than 96% missing data in the training dataset.
```{r,fig.keep='none'}
c(num_bad_in_testing=length(bad_cols),
     num_bad_in_training=sum(apply(is.na(df[,bad_cols]), 2, sum)/nrow(df) > 0.96))

#The following plot confirms the few data points which are valid do not strongly predict
#the "classe". This plot was omitted from the output for brevity.
#qplot(df[,bad_cols[1]], df[,"classe"])
```


#Modeling with Cross Validation
Now that we have identified a solid set of features, we're ready to begin modeling.

##Training and Testing Sets
```{r,cache=TRUE}
set.seed(1234)
inTrain <- createDataPartition(y=df$classe, p=0.80,list=FALSE)
training <- df[inTrain, -bad_cols]
# testing set will be used ONE TIME after model selection and parameter
# tuning are completed.
testing <- df[-inTrain, -bad_cols]

#the sensor columns
sensor_cols <- c(8:59)
```

We used an 80% and 20% split for training and testing sets. The testing set we refer to here was split off from the training data provided. *It will be set aside and used one time to predict our out-of-sample data rate.* We used a fixed seed in order to avoid accidentally training with the testing set when the partition is randomly re-created.

##Principal Components Analysis
```{r}
prepro_obj <- preProcess(training[,sensor_cols], method=c("center","scale"))
processed <- predict(prepro_obj, training[,sensor_cols])
plot(prcomp(processed)$sd)
```

After we scaled and centered the data, we looked at the plot of the standard deviations of its principal components. A lack of sharp drop off in variability suggests performing PCA may be of little use to our models. We will run an actual model with PCA to see if this hunch is correct.

##The Models
We take advantage of the paralles package to use multiple cores to speed up modeling. Through experimentation, we found using one less than the number of detected cores improves system responsiveness during the modeling process.

```{r}
restartClusters <- function() {
  if (exists("cl",mode="list")) try(stopCluster(cl))
  # leaving one core out improves system responsiveness
  cl <<- makeCluster(detectCores()-1)
  registerDoParallel(cl)
}

```

### Generalized Boosted Models
The first set of models we run are Generalized Boosted Models. We run GBM under different options such as bootstrap versus 10-fold cross-validation, no preprocessing vs center and scaling vs Principal Component Analysis.

```{r,cache=TRUE,results='hide'}
restartClusters()
proc1 <- system.time(mod1 <- train(training[,sensor_cols], training[,"classe"], 
                                      method="gbm"))
```

```{r,cache=TRUE,results='hide'}
cvControl <- trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,
  ## repeated ten times
  repeats = 1)
restartClusters()
proc2 <- system.time(mod2 <- train(training[,sensor_cols], training[,"classe"], 
                                      method="gbm", trControl = cvControl))
```

```{r,cache=TRUE,results='hide'}
restartClusters()
proc3 <- system.time(mod3 <- train(training[,c(2,sensor_cols)], training[,"classe"], 
                                      method="gbm", trControl = cvControl))
```

```{r,cache=TRUE,results='hide'}
restartClusters()
proc4 <- system.time(mod4 <- train(training[,sensor_cols], training[,"classe"], 
                                      preProcess=c("center","scale"),
                                      method="gbm", trControl = cvControl))
```

```{r,cache=TRUE,results='hide'}
restartClusters()
proc5 <- system.time(mod5 <- train(training[,sensor_cols], training[,"classe"], 
                                      preProcess=c("pca"),
                                      method="gbm", trControl = cvControl))
```

### Random Forrests
Next we run Random Forrest models comparing boostrap vs. 10-fold cross validation, as well as introducing user\_name and and num\_window variables as predictors.

```{r,cache=TRUE,results='hide'}
restartClusters()
proc6 <- system.time(mod6 <- train(training[,sensor_cols], training[,"classe"], 
                                      method="rf"))
```

```{r,cache=TRUE,results='hide'}
restartClusters()
proc7 <- system.time(mod7 <- train(training[,sensor_cols], training[,"classe"], 
                                      method="rf", trControl = cvControl))
```
```{r,cache=TRUE,results='hide'}
restartClusters()
proc8 <- system.time(mod8 <- train(training[,c(2,sensor_cols)], training[,"classe"], 
                                      method="rf"))
```
```{r,cache=TRUE,results='hide'}
restartClusters()
proc9 <- system.time(mod9 <- train(training[,c(2,7,sensor_cols)], training[,"classe"], 
                                      method="rf"))
```
```{r}
models <- NULL

models <- rbind(models, list(method="gbm", CV="bootstrap", 
                             time_used=proc1["elapsed"], 
                             notes="", model=mod1))
models <- rbind(models, list(method="gbm", CV="10-fold", 
                             time_used=proc2["elapsed"], 
                             notes="", model=mod2))
models <- rbind(models, list(method="gbm", CV="10-fold", 
                             time_used=proc3["elapsed"], 
                             notes="user_name", model=mod3))
models <- rbind(models, list(method="gbm", CV="10-fold", 
                             time_used=proc4["elapsed"], 
                             notes="center,scale", model=mod3))
models <- rbind(models, list(method="gbm", CV="10-fold", 
                             time_used=proc5["elapsed"], 
                             notes="PCA", model=mod3))

models <- rbind(models, list(method="rf", CV="bootstrap", 
                             time_used=proc6["elapsed"], 
                             notes="", model=mod6))
models <- rbind(models, list(method="rf", CV="10-fold", 
                             time_used=proc7["elapsed"], 
                             notes="", model=mod7))
models <- rbind(models, list(method="rf", CV="bootstrap", 
                             time_used=proc8["elapsed"], 
                             notes="user_name", model=mod8))
models <- rbind(models, list(method="rf", CV="bootstrap", 
                             time_used=proc9["elapsed"], 
                             notes="user_name,num_window", model=mod9))

res <- t(sapply(models[1:5,"model"], function(x) tail(x[["results"]],1)[c("Accuracy","AccuracySD")]))

res <- rbind(res, t(sapply(models[6:9,"model"], function(x) {
with(x$model, filter(results, mtry==bestTune[1,1])[c("Accuracy","AccuracySD")])  
})))

cbind(models[,-ncol(models)], res)
```
#Results

Boxplots are drawn where the whiskers correspond to the standard deviation of the accuracy across different cross-validation tests within each model. 


